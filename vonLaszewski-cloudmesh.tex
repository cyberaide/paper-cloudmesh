\documentclass{tex/sig-alternate-2013}

\usepackage[T1]{fontenc}
\usepackage{comment}
\usepackage{adjustbox}
\usepackage[usenames,dvipsnames]{color}
\usepackage{color}
\usepackage{textcomp}
\usepackage{mathabx}
%\usepackage{enumerate}
\usepackage{hyperref} 
\usepackage{array} 
\usepackage{graphicx} 
\usepackage{booktabs} 
\usepackage{pifont} 
\usepackage{rotating} 
\usepackage{color} 
\usepackage{tabularx} 
\usepackage{amssymb}
\usepackage{enumitem}
 
\newcommand*\rot{\rotatebox{90}} 

\newcommand{\todo}[1]{{\color{red}{#1}}}
\newcommand{\READ}{\todo{READ}}

\newfont{\mycrnotice}{ptmr8t at 7pt}
\newfont{\myconfname}{ptmri8t at 7pt}
\let\crnotice\mycrnotice%
\let\confname\myconfname%

\permission{Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.}
\conferenceinfo{BigSystem 2014,}{June 23, 2014, Vancouver, BC, Canada.}
\copyrightetc{Copyright 2014 ACM \the\acmcopyr}
\crdata{978-1-4503-2909-5/14/06\ ...\$15.00.\\
http://dx.doi.org/10.1145/2609441.2609638}

\clubpenalty=10000 
\widowpenalty = 10000

\begin{document}

\title{Accessing Multiple  Clouds with Cloudmesh} 
\numberofauthors{5} 
\author{
% 1st. author
\alignauthor
Gregor von Laszewski\titlenote{Corresponding author.}\\
       \affaddr{School of Informatics and Computing, Indiana University}\\
       \affaddr{919 E. 10th Street}\\
       \affaddr{Bloomington IN 47408, U.S.A.}\\ 
       \email{laszewski@gmail.com} 
% 2nd. author
\alignauthor
Fugang Wang \\
       \affaddr{School of Informatics and Computing, Indiana University}\\
       \affaddr{919 E. 10th Street}\\
       \affaddr{Bloomington IN 47408, U.S.A.}\\ 
% 3rd. author
\alignauthor Hyungro Lee\\
       \affaddr{School of Informatics and Computing, Indiana University}\\
       \affaddr{919 E. 10th Street}\\
       \affaddr{Bloomington IN 47408, U.S.A.}\\ 
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Heng Chen\\
       \affaddr{School of Informatics and Computing, Indiana University}\\
       \affaddr{919 E. 10th Street}\\
       \affaddr{Bloomington IN 47408, U.S.A.}\\ 
% 5th. author
\alignauthor Geoffrey C. Fox\\
       \affaddr{School of Informatics and Computing, Indiana University}\\
       \affaddr{919 E. 10th Street}\\
       \affaddr{Bloomington IN 47408, U.S.A.}\\ 
}

\date{27 April 2014}

\maketitle
\begin{abstract}

We present the design of a toolkit that can be employed by users and
administrators to manage virtual machines on multi-cloud environments.
It can be run by individual users or offered as a service to a shared
user community. We have practically demonstrated its use as part of a
FutureGrid service, allowing users of FutureGrid to utilize such a
service. Furthermore, we discuss implications and solutions for a
unified metrics system assisting the users to find and utilize
resources appropriate for their applications. Lastly, we discuss how
to move such a multi-cloud environment forward by integrating clouds
managed by the community or are offered as public clouds. This
includes the introduction of a mutual trust agreement on a user and
project basis. We have developed a number of components that support
the creation of such a multi-cloud environment. This system is known
as Cloudmesh and has been used in practice to achieve virtual machine
management in multiple clouds. An important distinguishing factor of
Cloudmesh is that it also allows the use of bare metal provisioning
for supporting service providers and authorized users, offering
services beyond those available by typical clouds.

\end{abstract}

% A category with the (minimum) three required fields
\category{C.2.4}{Distributed Systems}{Cloud Computing}

\terms{Computer-Communication Networks}

\keywords{Cloud, multi-cloud, federated clouds, FutureGrid, Cloudmesh}

\section{Introduction}

Cloud computing has become an integral factor for managing
infrastructure by research organizations and industry. Users and
organizations are faced with a variety of solutions that may support
their needs. Such offerings include a variety of Infrastructure as a
Service (IaaS) frameworks. The choice between them provides a
significant risk of investment and has to be conducted carefully. The
community has the choice to either use public clouds or set up their
own private clouds. Public clouds are offered by large providers such
as Amazon, Microsoft, Google, Rackspace, HP, and others. Private
clouds are set up by internal Information Technology (IT) departments
and made available as part of the general IT infrastructure. Two
aspects resulting from this availability bear consideration. First,
which of the many Infrastructures should be chosen? Second, can the
multitude of IaaS service offerings be leveraged and instead of
choosing one, can a service be offered that utilizes multiple
services? Furthermore, we have to consider that the IaaS frameworks to
deploy private clouds are evolving and that extensions may be needed
that are not yet offered by the frameworks to adequately address the
requirements posed by advanced IT service infrastructures of research
organizations.

In this paper, we will present the design and implementation of a
toolkit that addresses some of the points raised here.  The paper is
structured as follows. First, we discuss the important terminology
used in this paper (Section \ref{S:terminology}).  Next, we analyze
the requirements in more detail (Section \ref{S:requirements}) and
summarize some related work (Section \ref{S:related}).  Then, we
explore the requirements that motivate our design (Section
\ref{S:design}).
We focus on two aspects of the Cloudmesh features, namely dynamic
provisioning/ rain. Last, we present our conclusion (Section
\ref{S:conclusion}).



\subsection{Terminology} \label{S:terminology}

We will be using the following definitions throughout the paper:

\begin{description}[leftmargin=*,itemsep=0pt,topsep=0pt]

\item[Public cloud:]  a service provider makes resources available to users over the public internet. This includes compute, storage, and applications. FutureGrid offers a public cloud to its users. 

\item[Private cloud:] access to services may have additional restrictions. Restrictions could include a limited set of authorized users to the services offered or possible restrictions regarding exposing services on the public internet. FutureGrid offers the ability to set up private clouds for special projects. Examples include modified OpenStack deployments or reserved resources for classes.

\item[Hybrid cloud:] a combination of public and private clouds. 

\item[Multi-cloud:] access to a number of different clouds that may even use different IaaS or PaaS offerings. 

\item[HPC service:] a cloud service that allows the ability to run high performance computing jobs, for example on a compute cluster offering MPI. 

\item[Provisioning:] a set of instructions to install the operating system, data and software to enable access to it. 

\item[Rain:] an advanced set of instructions that not only provisions the operating system, but allows the deployment and configuration of useful and complex services to be run on one or multiple machines in order to provide a service utilizing potentially distributed resources or services.  It also contains the ability to re-provision servers and services, that is, services may be suspended and the resources used to run the service may be used by other services.

\item[Provider consortium:] is a (virtual) organization that integrates resources from multiple providers. We also can refer to such a consortium as a multi-cloud Grid.

\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}\label{S:related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


It is not possible to provide an extensive overview of related
research due to space limitations of this paper. Instead, we will
provide examples of activities that address similar although not the
same issues as our work.

\begin{description}[leftmargin=0pt,itemsep=0pt,topsep=0pt]

\item[Phantom] \cite{phantom12} is a tool that monitors
  the health of resources and automatically provisions and configures
  new ones based on demand. It is designed to automatically (a)
  provision new resources to counteract failures or (b) react to
  increasing demand, reducing constant attention and repetitive tasks
  that can be automated.  

  What makes our effort different, is that our framework is focused
  not only on the user initiated federation of resources, the access
  to the native protocols that are not only the reliance on EC2 EC2
  protocol, as well as the concept of access to baremetal
  provisioning. Furthermore, our framework includes the ability to
  provide a holistic cloud usage service that can be used to develop
  holistic scheduling algorithms for better utilization.

\item[RightScale] \cite{Rightscale} enables users to manage multi-cloud infrastructure by migrating workloads between private clouds and public clouds. RightScale interfaces with a wide variety of clouds including Amazon Web Services (AWS), Rackspace Cloud, Windows Azure, and Google Compute Engine. It also offers a cloud cost estimator, allowing customers to assess expenses they are charged by comparing their workload on various cloud providers.

Our effort is different because it is an open source toolkit and allows the deployment not only as a hosted service managed by one entity, but also allows the deployment by a provider, provider consortium, or even the user. 

%\item[StarCluster] \cite{www-starcluster} StarCluster is a toolkit to
%  simplify the process of building, configuring, and managing clusters
%  of virtual machines on Amazon's EC2 cloud. 

\end{description}


Other research efforts include theoretical definitions for cloud
federation \cite{kurze2011cloudfederation} or are relying on a single
IaaS, such as efforts planned for future versions of
OpenStack. Standard efforts provide an interesting approach to
multi-cloud interoperability, but at the same time may hinder the
innovation brought forward by individual IaaS offerings. We see such
an encumbrance with the difference between the OpenStack and EC2
API's, which leads to limitations in the functionality of Phantom.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{FutureGrid}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many of the requirements for this project originate from
FutureGrid. FutureGrid \cite{las2010gce,las12fg-bookchapter} is a
project to develop a high performance grid testbed that will allow
scientists to collaboratively develop and test innovative approaches
to parallel, grid, and cloud computing. It provides infrastructure and
advanced services that are documented in more detail in
\cite{vonLaszewski-bigdata-bookchapter2014} . FutureGrid users can
deploy their own hardware and software configurations on a
public/private cloud, and run their experiments. FutureGrid provides
an advanced framework to manage user and project affiliation and
propagates this information to a variety of subsystems constituting
the FutureGrid service infrastructure. This includes operational services to
deal with authentication, authorization and accounting. In particular
we have developed a unique metric framework
\cite{las08federated-cloud} that allows us to create usage reports
from our entire IaaS frameworks and is
presented in more detail later. Repeatable experiments can be created
with a number of tools including Pegasus, Precip and
Cloudmesh. Provisioning of services and images can be conducted by
Rain \cite{imagemanagement,fg-1295}.
One of the main features of FutureGrid is to offer its users a variety
of IaaS frameworks
\cite{comparisoncloud} including OpenStack, Eucalyptus,
and Nimbus. Based on our experience with
FutureGrid over the last couple of years, it is advantageous to offer
a mixed operation model. This includes a standard production cloud
that operates on-demand, but also a set of cloud instances that can be
reserved for a particular project. 


%We have conducted this for several projects in FutureGrid, including those that required dedicated access to resources as part of big data research such as classes \cite{fg405,fg368} or research projects with extremely large virtual machines \cite{fg298}.


%\cite{nagios} \cite{ganglia} \cite{inca}  \cite{las08federated-cloud}.

\begin{comment}

    \subsection{Federation}

    Cloud federation

    Account based federation

    Administrative federation

    \url{http://www.egi.eu/infrastructure/cloud/}

    \cite{kurze2011cloudfederation}

\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Requirements} \label{S:requirements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We list here a subset of requirements that are to be addressed by the
design and implementation of our toolkit. This includes (a) provide
virtual machine management in a multi-cloud environment while using
(a.1) FutureGrid resources, (a.2) external clouds from fresearch
partners, (a.3) public clouds; (b) provide multi-cloud services
controled by the user; (c) provide multi-cloud services controlled by
the provider; (d) enable deployment of the service by users or
providers; (e) enable raining (e.1) of operating systems (bare-metal
provisioning), (e.2) services, (e.3) platforms, (e.4) IaaS; (f)
monitoring infrastructure accross a multi-cloud environment; (g)
provid multiple interfaces such as (g.1) command line, (g.2) command shell, (g.3) REST,
(g.4) Python API; (h) deliver an (h.1) open source (h.2) extensible (h.3) easy
deployable, (h.4) documented toolkit.

As open source project much of the presented material in the next
sections can be found in its online Web page
\cite{github-cloudmesh}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design}\label{S:design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our initial design is addressing the requirments listed in
Section~\ref{S:requirements} and will provide a
tightly integrated software infrastructure toolkit addressing the need to
deliver a software-defined system encompassing virtualized and
bare-metal infrastructure, networks, application, systems and platform
software with a unifying goal of providing Cloud Testbeds as a Service
(CTaaS). This system is termed Cloudmesh to symbolize

\begin{enumerate}[leftmargin=*,itemsep=0pt,topsep=0pt]

\item the creation of a tightly integrated mesh of serviceto sys targeting
  multiple IaaS frameworks (Requirement (a)), 

\item the ability to federate a number of resources from academia and
  industry. This includes existing FutureGrid infrastructure, Amazon
  Web Services, Azure, HP Cloud, Karlsruhe using not only one IaaS
  framework but various (Requirements (a), (b), (c)),

\item the creation of an environment in which it becomes more easy to
  experiment with platforms and software services while assisting to
  deploy them more easily (Requirement (e)).  

\item the exposure of information to guide the efficient utilization
  (Requiremnet (f))

\end{enumerate}

In addition to virtual resources, Cloudmesh exposes baremetal
provisioning to users. Services will be available through command line,
API, and Web interfaces (Requirement (g)).

Due to its integrated services Cloudmesh provides the ability to be an
onramp for other clouds. It also provides information services to
various system level sensors to give access to sensor and utilization
data. Internally, it can be used to optimize the system usage. The
provisioning experience from FutureGrid has taught us that we need to
provide the creation of new clouds, the repartitioning of resources
between services (cloud shifting), and the integration of external
cloud resources in case of over provisioning (cloud bursting). As we
deal with many IaaS frameworks, we need an abstraction layer on top of
the IaaS framework. Experiment management is conducted with workflows
controlled in shells \cite{cmd3}, Python/iPython, as well as systems
such as OpenStack's Heat. Accounting is supported through additional
services such as user management and charge rate management. Not all
features are yet implemented. Figure \label{F:cm-func} shows the main
functionality that we target at this time to implement.


\begin{figure}[htb]
  \centering
    \includegraphics[width=1.0\columnwidth]{images/cm-functionality.pdf}
  \caption{Cloudmesh Functionality.}\label{F:cm-func}
\end{figure}

To implement the Cloudmesh functionality, we have devised a layered
architecture to gradually provide services in support of the
requiremennts identified for our toolkit. 

The architecture is depicted in Figure~\ref{F:arch} and conatains
threem main layers: the resource layer, the provisioning and execution
layer, and the management framework layer.

\begin{comment}
 a Cloudmesh Management Framework for monitoring and operations, user and project management, experiment planning and deployment of services needed by an experiment, provisioning and execution environments to be deployed on resources to (or interfaced with) enable experiment management, and resources.
\end{comment}

\begin{figure}[htb]
  \centering
    \includegraphics[width=1.0\columnwidth]{images/cm-arch.pdf}
    \vspace{-24pt}
  \caption{Cloudmesh Architecture.}\label{F:arch}
\end{figure}

\subsection{Management Framework Layer}

\begin{description}[leftmargin=0pt,itemsep=0pt,topsep=0pt]

\item[User and Project Services.] FutureGrid user and project services
  simplify the application processes needed to obtain user accounts
  and projects. We have demonstrated in FutureGrid the ability to
  create accounts in a very short time, including vetting projects and
  users -- allowing fast turn-around times for the majority of
  FutureGrid projects with an initial startup allocation. Cloudmesh
  reuses this infrastructure and also allows users to manage proxy
  accounts to federate to other IaaS services to provide an easy
  interface to integrate them.

\item[Accounting and App Store.] To lower the barrier of entry,
  Cloudmesh will be providing a shopping cart, which will allow
  checking out of predefined repeatable experiment templates. A cost
  is associated with an experiment making it possible to engage in
  careful planning and to save time by reusing previous
  experiments. Additionally, the Cloudmesh App Store may function as a
  clearing-house of images, image templates, services offered and
  provisioning templates. Users may package complex deployment
  descriptions in an easy parameter/form-based interface and other
  users may be able to replicate the specified setup.  Due to our
  advanced Cloudmesh Metrics framework we are in the position to
  further develop an integrated accounting framework allowing a usage
  cost model for users and management to identify the real impact of
  an experiment on resources. This will help avoid
  overprovisioning and inefficient resource usage. The cost model will
  be based not only on number of core hours used, but also the
  capabilities of the resource, the time, and special support it takes
  to set up the experiment. We will expand upon the metrics framework
  of FutureGrid that allows measuring of VM and HPC usage and
  associate this with cost models. Benchmarks will be used to
  normalize the charge models.

\end{description}

\subsection{Provisioning and Execution Layer}

\begin{description}[leftmargin=0pt,itemsep=0pt,topsep=0pt]

\item[Baremetal Provisioning.] We have a broad vision of resource
  integration offering different levels of control from bare
  metal to use of a portion of a resource. As part of our efforts we
  originally developed a provisioning framework based on XCAT. However
  due to limitations and significant changes between versions we are
  currently replacing it with a more general framework that allows the
  utilization of different bare metal provisioners. At this time we
  have provided an interface for cobbler and are also targeting an
  interface to OpenStack Ironic.

\item[Virtual Machine Provisioning.] Cloudmesh provides an abstraction
  layer to allow the integration of virtual machine management APIs
  based on the native IaaS service protocols. This helps in exposing
  features that are otherwise not accessible when quasi protocol
  standards such as EC2 are used on non-AWS IaaS frameworks. It also
  prevents limitations that exist in current implementations, such as
  libcloud to use OpenStack.

\item[Network Provisioning.] Likewise, we must utilize networks
offering various levels of control, from standard IP connectivity to
completely configurable SDNs as novel cloud architectures will almost
certainly leverage NaaS and SDN alongside system software and
middleware. FutureGrid resources will make use of SDN using OpenFlow
whenever possible though the same level of networking control will not be
available in every location.

\item[Storage Provisioning.] As we have access to baremetal
  provisioning storage, provisioning becomes a possibility while
  keeping partions between deployments and experiments. However, we
  will have to expand upon making storage provisioning accessible to
  the users.

\item[Platform, IaaS, and Federated Provisioning.] One of the
  significant features of the design of Cloudmesh is that it can not only
  provision the operating system, but that through additional services
  it can also provision platforms, IaaS, and even federated
  services. This is achieved by the integration of cloudmesh shell
  scripting, or the utilization of DevOps frameworks such as Chef or Puppet. 

  Furthermore, we have already demonstrated via the Rain tool in
  FutureGrid that it is possible to {\em shift} resources
  allocations between services such as HPC and OpenStack or
  Eucalyptus. We are currently expanding upon this idea and developing
  intuitive user interfaces as part of Cloudmesh that assist
  administrators and users through role and project based
  authentication to move resources from one service to another (see
  Figure \ref{F:shift}).

\end{description}

\begin{figure}[htb]
  \centering
    \includegraphics[width=1.0\columnwidth]{images/shift2.pdf}
  \caption{Shifting resources makes it possible to offer flexibility
    in the service distribution in case of over or underprovisioning.}\label{F:shift}
\end{figure}

\subsection{Resource Layer}

To integrate IaaS frameworks Cloudmesh offers two
distinct services: (a) a federated IaaS frameworks hosted on
FutureGrid, (b) the availability of a service that is hosted on
FutureGrid, allowing the integration of IaaS frameworks through user
credentials either registered by the users or automatically obtained
from our distributed user directory.

For (b) several toolkits exist to create user-based federations,
including our own abstraction level which supports interoperability
via libcloud, but more importantly it directly supports the native
OpenStack protocol and overcomes limitations of the EC2 protocol and
the libcloud compatibility layer. Plugins that we currently develop
will enable access to clouds via firewall penetration, abstraction
layers for clouds with few public IP addresses and integration with
new services such as OpenStack Heat. We successfully federated
resources from Azure, AWS, the HP cloud, Karlsruhe Institute of
Technology Cloud, and four FutureGrid clouds using various versions of
OpenStack and Eucalyptus. The same will be done for OpenCirrus
resources at GeorgiaTech and CMU.  Additional management flexibility
will be introduced through automatic cloud-bursting and shifting
services. While cloud bursting will locate empty resources in other
clouds, cloud shifting will identify unused services and resources,
shut them down and provision them with services that are requested by
the users. We have demonstrated this concept in 2012 moving resources
for more than 100 users to services that were needed based on class
schedules. A reservation system will be used to allow for reserved
creation of such environments, along with improvements of automation
of cloud-shifting.


\subsection{Monitoring}

\begin{comment}
\paragraph{System Monitoring and Operations.}
The management framework contains services to facilitate FutureGrid
day-to-day operation, including federated or selective monitoring of
the infrastructure. Cloudmesh leverages FutureGrid for the operational
services and allows administrators to view ongoing system status and
experiments, as well as interact with users through ticket systems and
messaging queues in order to inform subscribed users on the status of
the system.






\paragraph{Monitoring}
\end{comment}


Cloudmesh must be able to access empirical data about the properties
and performance of the underlying infrastructure beyond what is
available from commercial cloud environments. The component of
Cloudmesh accomplishing this is called {\em Cloud Metrics}.


\begin{comment}

To accommodate this
requirement we have developed a uniform access interface to virtual
machine monitoring information available for OpenStack, Eucalyptus,
and Nimbus. In the future, we will be enhancing the access to
historical user information. Right now they are exposed through
predefined reports that we create on a regular basis. To achieve this
we will also leverage the ongoing work while using the AMPQ
protocol. Furthermore, Cloudmesh will provide access to common
monitoring infrastructure as provided by Ganglia, Nagios, Inca,
perfSonar, PAPI and others.
\end{comment}


Experience with FutureGrid has provided greater understanding about
resource allocation and utilization. We have learned that it is
essential to provide users and administrators with a holistic view of
the infrastructure in order to guide better utilization overall and on
an individual basis. This is crucial in cloud deployments such as
FutureGrid, which supports more than 380 projects and 2300 users (as
of April 2014). Due to such a large user base, resources could become
over-provisioned or not properly utilized by the users. Among the many
services that FutureGrid offers, we have particularly
focused on IaaS including OpenStack, Eucalyptus, Nimbus, as well as
batch systems, to offer high performance computing capabilities.
However, for this paper we will restrict our discussion to the IaaS
based monitoring components.  Other HPC related activities in regards
to monitoring and metrics are discussed in \cite{las13xdmod}.

%\cite{ubmod,las12xdmod-kernel,las12xdmod-planing,las13xdmod,smith13info}

When FutureGrid initially started, the existing IaaS frameworks such as Eucalyptus, Nimbus, and OpenStack did not provide adequate support for monitoring resource usage. Furthermore, a service with sufficient monitoring capabilities across heterogeneous cloud IaaS frameworks was not available. Hence, it was difficult to assess user utilization in a holistic fashion. Additionally, we found that some IaaS frameworks such as Nimbus, lacks support for project allocations, which is a must have feature to support project managed allocations as is the case of almost every modern shared datacenter.  To overcome these missing features and service offerings, we developed a {\em federated cloud metric service} that aggregates the information from distributed clusters and a variety of heterogeneous IaaS services, such as OpenStack, Eucalyptus, and Nimbus. We have named this service {\em Cloudmesh Metrics}.

The main components of {\em Cloudmesh Metrics} enable (a) the measurement of the resource allocation across several IaaS platforms, (b) the generation of data in regards to utilization, (c) the comparison of data via definable metrics to mine the usage statistics, (d) the display of the information through a convenient user interface, (e) the availability of a simple command line interface and shell language, and (f) the automatic creation of resource reports in printed format for arbitrary time periods.

The services offered by Cloudmesh Metrics support requirements from a variety of user communities. This includes individual users and users as part of projects (Section \ref{S:user-metric}), as well as administrative users (Section \ref{S:resource-metric}).



\subsubsection{User- and Project-based Metrics Services}\label{S:user-metric} 

In order for users to use a variety of clouds, it is important for them to monitor and compare their resource utilization. In case the usage is organized as a project, the project related information needs to be exposed while being able to clearly distinguish between different projects. Therefore, we need to support an overall project view.  Thus, the requirement exists to present the data to the user based on individual user utilization, group utilization, or even experiment utilization, where a particular experiment is analyzed instead of just looking at the total utilization.

\subsubsection{Resource Provider-based Metrics Services} \label{S:resource-metric}

A resource provider need to have access to a holistic view of the variety of metrics across the different clouds that build the multi-cloud environment as part of a provider consortium.  Summary information may be customized based on the requirements by individual users, project leads, resource providers, site managers, and funding agencies. This information is typically restricted to the actual {\em resources} for which administrative access exists in order to provide a holistic set of metrics.

\subsubsection{Metric Access for Multi-cloud Environments} 

Due to the different governance models between a private cloud managed as part of a provider consortium, and the integration of resources, use must be based on an access integration policy. In order to devise such a policy, we need to be aware of the hierarchical access management employed in clouds as depicted in Figure \ref{F:metric-hierarchy}.

\begin{figure}[htb]
  \centering
    \includegraphics[width=1.0\columnwidth]{images/metric-hierarchy.pdf}
  \caption{An example of a policy to establish a access public clouds as part of 
    provider and user managed multi-clouds.}
  \label{F:metric-hierarchy}
\end{figure}


\paragraph{Integration into a Provider Consortium Managed Multi-Cloud}

\newcommand{\ARROW}{$\Rightarrow~$}

In the hierarchy, we distinguish users \ARROW project with many users \ARROW a resource serving many projects \ARROW a provider having many resources \ARROW and a provider consortium with many providers.

The provider consortium has a multitude of possibilities to extend their resource offerings to its users while integrating public clouds. This could include replicating projects on public clouds, or assigning a particular user access to an account that integrates resources in a public cloud. In case multiple clouds are offered, this integration could be replicated for them. Hence, it would be possible for a provider consortium not only to provide private clouds as part of a multi-cloud environment, but also public cloud offerings allowing access to these public clouds through a provider consortium managed project or users on the public clouds. This is of particular interest if we intend to gain financial advantages through volume discounts that would otherwise not be available to the users.  In Figure \ref{F:metric-hierarchy} we show one example for such an integration policy where we simply map the existing projects and users of the consortium to projects and users of a public cloud. An applicable instance where such integration is easy to accomplish is the HP Cloud environment that uses OpenStack as its IaaS framework. Due to OpenStack's well-documented interfaces, it is possible to replicate the user and project information and provide detailed charges to the users on projects in case the HP OpenStack cloud would be used. Naturally, it would be possible to devise other integration policies such as restricting access to only approved projects, or provide limited access into different levels of the hierarchy.


\paragraph{Integration into a User Managed Multi-Cloud}

Based on our experience with FutureGrid, we must also be aware that users may have their own accounts and access to other clouds, thereby needing to integrate them into a multi-cloud environment. The user decides whether the metric information to such clouds is forwarded to the multi-cloud environment offered as part of a Provider Consortium. However, in most cases the user will not share this information. Hence, the metric system ideally should be able to allow users to integrate their own information into such a metric system in order for the user to gain a more accurate picture about their own cloud usage not just in the consortium, but also in the public clouds. We represent this clear division in Figure \ref{F:metric-hierarchy}. Explicit access policies must be defined to allow the users or projects to access the information provided by the public clouds.

\paragraph{Cloudmesh Metric Architecture}

The Cloudmesh metric architecture is based on the integration of an authorized REST service, that utilizes a simple abstraction layer to interface with the various cloud services to obtain needed information gathered under authorization constraints. The data will be hosted in a NOSQL database to allow mining of the data in map/reduce frameworks. Data can be ingested either directly through the database via the API, or through REST calls that are mitigated through message queues with AMPQ. Adapters can be written to integrate new information providers for other clouds. Policies can be used to limit the amount of information presented to other users or projects.

\subsubsection{Cloudmesh Metric Service for FutureGrid}

To work towards the goal of a metric system for multi-cloud environments, we started by basing our initial development efforts on the extension of the cloud metrics services that have been developed by FutureGrid for a multi-cloud environment for resource providers within FutureGrid. Here, we have limited the services to a provider consortium that offers information of clouds directly managed by the consortium. This includes OpenStack, Eucalyptus, and Nimbus clouds on various resources. The information access policy for using the resources is public, as this is most suitable to the goals of FutureGrid as a public testbed.

\paragraph{Data Collector and Metric service}

%\begin{figure}[htb]
%  \centering
%    \includegraphics[width=1.0\columnwidth]{images/cloudmesh-metrics.pdf}
%  \caption{Design of Cloudmesh Metrics Architecture}
%  \label{F:metric-arch}
%\end{figure}

One of the fundamental services needed is a data collector. It collects relevant data from a variety of sources including resource databases, log files, and data reporters. Hence, to integrate new cloud services into the data collectors we have to define a data model, as well as data sources that populate the data model with data. Currently, data collectors are available for OpenStack, Eucalyptus, and Nimbus but not limited to these platforms. Dependent on the IaaS framework they obtain the data from different sources such as log files or databases as listed in Table \ref{T:compare-iaas}. Beneficial information to be collected includes detailed information about the virtual machines (VM), the users and/or projects starting them, memory usage over the lifetime of the VMs, errors associated with VMs during runtime, or at startup. One of the issues to be addressed is, if such data should be directly accessed in the production environment offered by the IaaS framework. Practical experience with FutureGrid has shown that the analysis of the data poses a significant amount of stress on the originating resource, making it impractical to offer a detailed report and metric system on the original data sources. Hence, it is important that we replicate it when the information we request is involved in a detailed analysis. For some smaller scale queries, as the one posed by most users, direct access is sufficient and desirable for the live view of the system in order to provide information about how many VMs are currently running, on which system and by whom.

%\newcommand{\YES}{\checkmark}
%\newcommand{\NO}{\textopenbullet}
\newcommand{\YES}{\ding{51}}
\newcommand{\NO}{\ding{55}}
%\newcommand{\YES}{$\oplus$}
%\newcommand{\NO}{$\ominus$}


\begin{table}[h!]
  \caption{Measurement of IaaS on FutureGrid}\label{T:compare-iaas}
  ~\\
  \begin{small}
  \begin{tabularx}{\columnwidth}{|l|X|X|X|}
  \hline
                 & {\bf Nimbus} & {\bf OpenStack} & {\bf Eucalyptus} \\
    \hline
    \hline
    \multicolumn{4}{|l|}{\bf Documentation of the Data Sources} \\
    \hline
       & \NO & \YES & \YES \\
    \hline
    \hline
    \multicolumn{4}{|l|}{\bf Data Sources} \\
    \hline
         & sqlite3 & MySQL & Log Files \\
    \hline
    \hline
    \multicolumn{4}{|l|}{\bf Metrics} \\
    \hline
    ~~vCPU core & \YES & \YES & \YES \\
    ~~memory & \YES & \YES & \YES \\
    ~~disk & \YES & \YES & \YES \\
    ~~instance type   & \NO & \YES & \YES \\
    ~~host & \YES & \YES & \YES \\
    \hline
    \hline
    \multicolumn{4}{|l|}{\bf Account Management Features} \\
    \hline
    ~~Users     & \YES & \YES & \YES \\
    ~~Projects & \NO & \YES & \YES \\
    \hline
    \hline
    \multicolumn{4}{|l|}{\bf Cluster} \\
    \hline
    ~~Alamo  & \YES & \YES & \NO \\
    ~~Foxtrot & \YES & \NO & \NO \\
    ~~Hotel    & \YES & \YES & \NO \\
    ~~India     & \NO  & \YES & \YES \\
    ~~Sierra    & \NO & \YES & \YES \\
%    ~~Lima     & ?       &  ?      &  ?       \\   
    \hline
%    Region& \shortstack[l]{TACC$^1$, \\UF$^2$, \\UChicago$^3$, \\SDSC$^4$} & \shortstack[l]{TACC, \\IU$^5$, \\SDSC } & \shortstack[l]{IU, \\SDSC$^6$} \\
%    \hline
  \end{tabularx}\\
%  $^1$ Texas Advanced Computing Center \\
%  $^2$ University of Florida \\
%  $^3$ University of Chicago \\
%  $^4$ San Diego Supercomputing Center\\
%  $^5$ Indiana University\\
%  $^6$ in early 2014\\
\end{small}
\end{table}


\paragraph{Metric Analyzer}

The data collected provides the opportunity to analyze it for specific needs in a repeated fashion or provide filters and services for further specialized analysis. The FutureGrid metric framework therefore provides a metric analyzer component with a convenient interface for analyzing the data not only on an automated fashion, but also interactively through a simple metrics analyzer shell. Information of interest includes yearly, monthly, and/or weekly usage information by user, project, resource, provider, and the agglomerated information. Our scripting environment provides this information and is run at predefined intervals or upon request. In the future, we will be enhancing the service to allow users to schedule queries to conduct specific analysis. To avoid repeated analysis, metric result caching is conducted. Thus, if a query has been executed in the past the result is cached and returned without reanalysis (if not forced). To more easily facilitate fast and distributed calculation of the results by multiple users, we will base future versions of the Cloudmetric system on NoSQL database technologies.

\paragraph{Metric Interface}

Early on we recognized that the access of information and the metrics must be provided through a variety of interfaces. This includes command shells, programming API's, REST interfaces, graphical user interfaces, and printable reports.

\begin{description}[leftmargin=*,itemsep=0pt,topsep=0pt]

\item[Interactive Command Shell.] To simplify the interactive use, we have developed a python command shell called CMD3 that allows the dynamic load of additional commands, thus making it ideal to define new analytic methods on the fly if they are not provided by the original toolkit.

\item[REST API.] We are currently building access through a convenient REST API to allow easy access from Web frameworks, but also integration from arbitrary programming languages

\item[Programming API.] We have provided a robust API interface in python to access the basic analytical functions useful for many users and reused by the interactive command shell and the REST service.

\item[Graphical Representation and Printable Reports.] ~\\ Using our
  basic API and command shell, we have integrated them into the Python
  sphinx framework to expose the metric data in a convenient form and
  present the data online via charts or in a PDF report. As sphinx
  offers the export of data reports in PDF, we leverage this framework
  and do not have to develop a separate framework for it. The sphinx
  framework service is currently enhanced, allowing customizable
  interactive queries to the metric and data sources. The data can be
  represented visually in various chart forms such as bar graphs, line
  charts, or pie charts. A template for generating a quarterly and
  yearly report of the data exists making adaptation to additional
  resources or other provider consortia easy. Furthermore, the data
  can be exported in a variety of formats such as JSON or CSV making
  it possible to use other tools, such as excel for data post
  processing.  In Table~\ref{T:iaas-with-graph} we are including a
  limited number of examples to demonstrate the various data
  representations of the Cloudmetric system that are exposed to the
  users.

\end{description}

\begin{table}[t]
  \caption{Metric visualization with graphs}\label{T:iaas-with-graph}
  \begin{center}
  \begin{small}
  \begin{tabular}{|p{0.2\columnwidth}|p{0.7\columnwidth}|}
  \hline
  {\bf Example} & {\bf Description} \\
    \hline
    \adjustbox{valign=t}{\includegraphics[width=0.2\columnwidth]{images/metric-line-chart.pdf}}
    &  
    Detailed display of virtual machine information including number
    of virtual machines, user count, memory utilization, disk
    utilization, project lead, etc. \\
    \hline
    \adjustbox{valign=t}{\includegraphics[width=0.2\columnwidth]{images/metric-histogram-chart.pdf}}
    & Summary information for periods to display aggregates of the
    metrics gathered by the system, such as number of VMs by month for
    a user, project, or resource.  \\
    \hline
    \adjustbox{valign=t}{\includegraphics[width=0.2\columnwidth]{images/metric-pie-chart.pdf}}
    & Alternate representation of aggregated information in pie
    charts. \\
    \hline
    \adjustbox{valign=t}{\includegraphics[width=0.2\columnwidth]{images/metric-table-chart.pdf}}
    & 
    Alternate representation of agglomerated information in a table
    (exporatble as csv or json). \\
    \hline
  \end{tabular}
\end{small}
\end{center}
\end{table}




\subsection{Graphical User Interface}

Despite the fact that Cloudmesh was originally a quite sophisticated
command shell and command line tool, we recently dedicated more time
in exposing this functionality through a convenient Web
interface. Some more popular views if this interface are depicted in
Figure \ref{F:instances} hinting on how easy it is with a single
button to create multiple VMs across a variety of IaaS. Notably, this
not only includes resources at IU but also at external
locations. Pushing this easy management in a more sophisticated
experience for the user while associating one-click deployments that
include the ability to deploy virtual clusters, Hadoop environments,
and other more elaborate setups we provide an early prototype
screenshot in Figure \ref{F:oneclick}.


\begin{figure}[p]
  \centering
    \includegraphics[width=1.0\columnwidth]{images/rainbow.pdf}
  \caption{Monitoring the service distribution in FutureGrid Clusters with Cloudmesh.}

    \includegraphics[width=1.0\columnwidth]{images/instances.pdf}
  \caption{Screenshot demonstrating how easy it is to manage multiple VMs across various clouds.}\label{F:instances}
  \centering
    \includegraphics[width=1.0\columnwidth]{images/oneclick.pdf}
  \caption{One click deployment of platforms and sophisticated
    services that could even spawn multiple resources.}\label{F:oneclick}
\end{figure}

\begin{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}\label{S:implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

TBD


\section{Code}

\begin{figure}[htb]
\begin{small}
\begin{verbatim}
class ComputeBaseType:
    flavors = {}, images = {}, servers = {}
    // Methods
    def __init__(self, label, cred=None):
    def connect(self):
    def get(self, type): type in {server, images, flavors, 
                                  users, tennant}
    def refresh(self, type=None):
    def vm_create(self, name=None,
                  flavor_name=None,
                  image_id=None,
                  security_groups=None,
                  key_name=None,
                  meta=None):
    def vm_delete(self, id):
    def rename(self, old, new, id=None):
    def usage(self, start, end, format='dict'):
    def limits(self):
    def wait(self, vm_id, vm_status, seconds=2):
    def __str__(self):
    def vms(self):
    def status(self, vm_id):
    def set_credentials(self, cred):
    def keypair_list(self):
    def keypair_add(self, keyname, keycontent):
    def keypair_remove(self, keyname):
\end{verbatim}
\end{small}
\vspace{-12pt}
\caption{Abstract interface to access different virtual machine
  management functionality}
\end{figure}
\end{comment}

\begin{comment}
\paragraph{Integration into XSEDE}
TBD
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Status and Future Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

At this time we have developed a first version of Cloudmesh and
focussed on the development of three of its components. This includes
virtual machine management in multi-clouds, cloud metrics in
multi-clouds, and bare metal provisioning. The next steps will include
the development of other components we discussed in our
design. Cloudmesh has been successfully used in FutureGrid. A GUI and a
Cloudmesh shell is available for easy usage by users. It has been used
by users while deploying it on their local machines, but it also has
been demonstrated as a hosted service. A REST interface to the
management functionality is under development. Cloudmesh is an open
source project. It uses python and Javascript. Although the current
Web related functionality is developed in flask it is possible to
transition it easily to other web frameworks such as web2py or
django. We consider the cloudmesh API a basis for such reuse in other
frameworks.  Up to date information is available at
\url{cloudmesh.futuregrid.org}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{S:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper we presented the design of a toolkit called Cloudmesh
that allows to access to multiple clouds through convenient
interfaces. This includes command line, a command shell, REST, as well
as a graphical user interface. Cloudmesh is under active development
and has shown its viability for accessing more than EC2 based
clouds. Native interfaces to OpenStack, Azure, as well as any EC2
compatible cloud have been delivered and virtual machine management
enabled. An important contribution of Cloudmesh is that it provides a
sophisticated interface to bare metal provisioning capabilities that
not only can be used by administrators, but also by authorized
users. A role based authorization service makes this possible.
Furthermore, we have developed a multi-cloud metrics
framework that leverages information from various IaaS
frameworks. Future enhancements will include network and
storage provisioning.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Acknowledgment 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\section{Acknowledgments}

This material is based upon work supported by the National Science
Foundation under Grant No. 0910812 and OCI 1025159. Some work of
Cloudmesh is based on earlier work conducted as part of \url{cyberaide.org}.

\bibliographystyle{abbrvurl}

%\bibliography{% 
%bib/vonLaszewski-jabref.bib,%
%bib/cyberaide-cloud,%
%bib/cyberaide-metric} 

\bibliography{vonLaszewski-cloudmesh}

\balancecolumns 
\end{document}
